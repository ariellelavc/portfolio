{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62758476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 21:37:39.994290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 21:37:40.855261: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lac/anaconda3/envs/deepchem_env/lib/\n",
      "2023-01-07 21:37:40.855390: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/lac/anaconda3/envs/deepchem_env/lib/\n",
      "2023-01-07 21:37:40.855398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58891b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99868e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 21:37:45.398219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.405631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.405938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.406656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 21:37:45.407349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.407675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.407933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.973525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.973791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.973993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-07 21:37:45.974145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4472 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "model = dc.models.RobustMultitaskClassifier(n_tasks=12, \n",
    "                                            n_features=1024, \n",
    "                                            layer_sizes=[1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da620d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 21:37:57.292305: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55e86214c170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-07 21:37:57.292329: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1060, Compute Capability 6.1\n",
      "2023-01-07 21:37:57.298031: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-07 21:37:57.443304: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'mean-roc_auc_score': 0.9661840010717327}\n",
      "Test set score: {'mean-roc_auc_score': 0.6735673068282931}\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, nb_epoch=10)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31466036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1ece60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DiskDataset X.shape: (6264,), y.shape: (6264, 12), w.shape: (6264, 12), task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n",
      "<DiskDataset X.shape: (783,), y.shape: (783, 12), w.shape: (783, 12), ids: ['N#C[C@@H]1CC(F)(F)CN1C(=O)CNC1CC2CCC(C1)N2c1ncccn1'\n",
      " 'CN(C)C(=O)NC1(c2ccccc2)CCN(CCC[C@@]2(c3ccc(Cl)c(Cl)c3)CCCN(C(=O)c3ccccc3)C2)CC1'\n",
      " 'CSc1nnc(C(C)(C)C)c(=O)n1N' ...\n",
      " 'O=C(O[C@H]1CN2CCC1CC2)N1CCc2ccccc2[C@@H]1c1ccccc1'\n",
      " 'C#C[C@]1(O)CC[C@H]2[C@@H]3CCC4=CC(=O)CC[C@@H]4[C@H]3C(=C)C[C@@]21CC'\n",
      " 'NC(=O)C(c1ccccc1)(c1ccccc1)[C@@H]1CCN(CCc2ccc3c(c2)CCO3)C1'], task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n",
      "<DiskDataset X.shape: (784,), y.shape: (784, 12), w.shape: (784, 12), ids: ['CC1(C)S[C@@H]2[C@H](NC(=O)Cc3ccccc3)C(=O)N2[C@H]1C(=O)O.CC1(C)S[C@@H]2[C@H](NC(=O)Cc3ccccc3)C(=O)N2[C@H]1C(=O)O.c1ccc(CNCCNCc2ccccc2)cc1'\n",
      " 'CC(C)(c1ccc(Oc2ccc3c(c2)C(=O)OC3=O)cc1)c1ccc(Oc2ccc3c(c2)C(=O)OC3=O)cc1'\n",
      " 'Cc1cc(C(C)(C)C)c(O)c(C)c1Cn1c(=O)n(Cc2c(C)cc(C(C)(C)C)c(O)c2C)c(=O)n(Cc2c(C)cc(C(C)(C)C)c(O)c2C)c1=O'\n",
      " ... 'CN[C@@H]1C[C@@H](c2ccc(Cl)c(Cl)c2)c2ccccc21'\n",
      " 'Cl/C=C\\\\C[N+]12CN3CN(CN(C3)C1)C2'\n",
      " 'NC(=O)c1ccc[n+]([C@@H]2O[C@H](COP(=O)([O-])OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)c1'], task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n",
      "['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n",
      "[<deepchem.trans.transformers.BalancingTransformer object at 0x7f69c05cdf10>]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)\n",
    "print(tasks)\n",
    "print(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faac2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = len(tasks)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e6e898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2595729446411133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks, \n",
    "                                 mode='classification', \n",
    "                                 batch_size=batch_size,\n",
    "                                 model_dir='../models')\n",
    "model.fit(train_dataset, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abb7c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function KerasModel._compute_model at 0x7f6701b5c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training set score: {'mean-roc_auc_score': 0.9731465873064765}\n",
      "Test set score: {'mean-roc_auc_score': 0.7095828758533238}\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6be156",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4cbbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.9169800e-01, 1.0830200e-01],\n",
       "        [9.9424642e-01, 5.7536471e-03],\n",
       "        [8.4670073e-01, 1.5329923e-01],\n",
       "        ...,\n",
       "        [5.6693423e-01, 4.3306577e-01],\n",
       "        [9.6280432e-01, 3.7195709e-02],\n",
       "        [9.6311665e-01, 3.6883343e-02]],\n",
       "\n",
       "       [[9.5142287e-01, 4.8577081e-02],\n",
       "        [9.9920100e-01, 7.9901505e-04],\n",
       "        [9.2275774e-01, 7.7242233e-02],\n",
       "        ...,\n",
       "        [4.7724807e-01, 5.2275199e-01],\n",
       "        [5.1812518e-01, 4.8187479e-01],\n",
       "        [1.5451497e-01, 8.4548503e-01]],\n",
       "\n",
       "       [[9.9851912e-01, 1.4808973e-03],\n",
       "        [9.9993205e-01, 6.7964538e-05],\n",
       "        [9.9085337e-01, 9.1466326e-03],\n",
       "        ...,\n",
       "        [8.1599164e-01, 1.8400839e-01],\n",
       "        [5.6524307e-01, 4.3475699e-01],\n",
       "        [9.8482311e-01, 1.5176857e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[9.5263785e-01, 4.7362126e-02],\n",
       "        [7.6178819e-01, 2.3821183e-01],\n",
       "        [3.4113908e-01, 6.5886092e-01],\n",
       "        ...,\n",
       "        [9.6663976e-01, 3.3360250e-02],\n",
       "        [8.4752208e-01, 1.5247795e-01],\n",
       "        [9.4517559e-01, 5.4824382e-02]],\n",
       "\n",
       "       [[9.8333251e-01, 1.6667455e-02],\n",
       "        [9.9643195e-01, 3.5680381e-03],\n",
       "        [9.9956399e-01, 4.3595542e-04],\n",
       "        ...,\n",
       "        [9.5623088e-01, 4.3769073e-02],\n",
       "        [9.4734699e-01, 5.2653052e-02],\n",
       "        [9.6705574e-01, 3.2944281e-02]],\n",
       "\n",
       "       [[9.8995435e-01, 1.0045604e-02],\n",
       "        [9.9772638e-01, 2.2736792e-03],\n",
       "        [4.9981520e-01, 5.0018483e-01],\n",
       "        ...,\n",
       "        [9.9817514e-01, 1.8248865e-03],\n",
       "        [9.9682057e-01, 3.1794037e-03],\n",
       "        [9.1364664e-01, 8.6353362e-02]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [toxic, nontoxic] for each class\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0fb562c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 12, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "875cb32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0.88619334 0.11380665]\n",
      " [0.9880189  0.01198121]\n",
      " [0.8082292  0.19177072]\n",
      " [0.97490966 0.02509035]\n",
      " [0.17723855 0.8227615 ]\n",
      " [0.92856276 0.07143722]\n",
      " [0.9923004  0.00769958]\n",
      " [0.49192783 0.5080722 ]\n",
      " [0.8349025  0.16509743]\n",
      " [0.9891577  0.01084228]\n",
      " [0.97145295 0.02854702]\n",
      " [0.97891515 0.0210848 ]]\n"
     ]
    }
   ],
   "source": [
    "y_true = test_dataset.y\n",
    "#print(y_true.shape)\n",
    "print(y_true[0:10])\n",
    "print(pred[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ea4ce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR-AR 0.7134889182445325\n",
      "NR-AR-LBD 0.721499828001376\n",
      "NR-AhR 0.7846035436039207\n",
      "NR-Aromatase 0.6635584168134183\n",
      "NR-ER 0.599779911964786\n",
      "NR-ER-LBD 0.7587842476440118\n",
      "NR-PPAR-gamma 0.7434979718444286\n",
      "SR-ARE 0.5909171883748154\n",
      "SR-ATAD5 0.7698018803211879\n",
      "SR-HSE 0.7270417737232597\n",
      "SR-MMP 0.7494246608527132\n",
      "SR-p53 0.6925961688514357\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.roc_auc_score\n",
    "cummulative_score = 0.\n",
    "for i in range(n_tasks):\n",
    "    score = metric(dc.metrics.to_one_hot(y_true[:,i]), pred[:,i])\n",
    "    print(tasks[i], score)\n",
    "    cummulative_score += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b08fd574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7095828758533238"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = cummulative_score/n_tasks\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daad442",
   "metadata": {},
   "source": [
    "#### GraphConvModel builing from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e32e7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "class CustGraphConvModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(CustGraphConvModel, self).__init__()\n",
    "    self.gc1 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm1 = layers.BatchNormalization()\n",
    "    self.gp1 = GraphPool()\n",
    "\n",
    "    self.gc2 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm2 = layers.BatchNormalization()\n",
    "    self.gp2 = GraphPool()\n",
    "\n",
    "    self.dense1 = layers.Dense(256, activation=tf.nn.tanh)\n",
    "    self.batch_norm3 = layers.BatchNormalization()\n",
    "    self.readout = GraphGather(batch_size=batch_size, activation_fn=tf.nn.tanh)\n",
    "\n",
    "    self.dense2 = layers.Dense(n_tasks*2)\n",
    "    self.logits = layers.Reshape((n_tasks, 2))\n",
    "    self.softmax = layers.Softmax()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    gc1_output = self.gc1(inputs)\n",
    "    batch_norm1_output = self.batch_norm1(gc1_output)\n",
    "    gp1_output = self.gp1([batch_norm1_output] + inputs[1:])\n",
    "\n",
    "    gc2_output = self.gc2([gp1_output] + inputs[1:])\n",
    "    batch_norm2_output = self.batch_norm1(gc2_output)\n",
    "    gp2_output = self.gp2([batch_norm2_output] + inputs[1:])\n",
    "\n",
    "    dense1_output = self.dense1(gp2_output)\n",
    "    batch_norm3_output = self.batch_norm3(dense1_output)\n",
    "    readout_output = self.readout([batch_norm3_output] + inputs[1:])\n",
    "\n",
    "    logits_output = self.logits(self.dense2(readout_output))\n",
    "    return self.softmax(logits_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92b4bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_model = dc.models.KerasModel(CustGraphConvModel(), loss=dc.models.losses.CategoricalCrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462c3fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepchem.feat.mol_graphs.ConvMol at 0x7f67193f9fd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ce12295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "def data_generator(dataset, epochs=1):\n",
    "  for ind, (X_b, y_b, w_b, ids_b) in enumerate(dataset.iterbatches(batch_size, epochs,\n",
    "                                                                   deterministic=False, pad_batches=True)):\n",
    "    multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "    inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "    for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "      inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "    labels = [to_one_hot(y_b.flatten(), 2).reshape(-1, n_tasks, 2)]\n",
    "    weights = [w_b]\n",
    "    yield (inputs, labels, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e7ca81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22766029357910156"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_model.fit_generator(data_generator(train_dataset, epochs=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f98372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'mean-roc_auc_score': 0.4656707930765321}\n",
      "Test set score: {'mean-roc_auc_score': 0.5067519229695513}\n"
     ]
    }
   ],
   "source": [
    "print('Training set score:', cust_model.evaluate_generator(data_generator(train_dataset), [metric], transformers))\n",
    "print('Test set score:', cust_model.evaluate_generator(data_generator(test_dataset), [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21326c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
